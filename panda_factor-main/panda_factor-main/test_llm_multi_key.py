"""
æµ‹è¯•å¤šAPIå¯†é’¥è´Ÿè½½å‡è¡¡å’Œæ•…éšœè½¬ç§»
"""

import sys
import os

current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, os.path.join(current_dir, "panda_common"))

print("=" * 70)
print("æµ‹è¯•LLMå¤šå¯†é’¥è´Ÿè½½å‡è¡¡")
print("=" * 70)
print()

try:
    from panda_common.config import config
    from panda_common.llm_manager import get_llm_manager
    
    print("[1/5] åˆå§‹åŒ–LLMç®¡ç†å™¨...")
    llm_manager = get_llm_manager(config)
    
    status = llm_manager.get_status()
    print(f"âœ… ç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
    print(f"  APIå¯†é’¥æ•°é‡: {status['total_keys']}")
    print(f"  è´Ÿè½½å‡è¡¡ç­–ç•¥: {status['strategy']}")
    print(f"  é»˜è®¤æ¨¡å‹: {status['default_model']}")
    print()
    
    print("[2/5] å¯ç”¨çš„é‡‘èåˆ†ææ¨¡å‹:")
    for model_type, model_name in status['available_models'].items():
        print(f"  {model_type}: {model_name}")
    print()
    
    print("[3/5] æµ‹è¯•DeepSeek V3ï¼ˆä»£ç åˆ†æèƒ½åŠ›ï¼‰...")
    messages = [
        {
            "role": "user",
            "content": "è¯·ç”¨ä¸€å¥è¯ä»‹ç»DeepSeek V3åœ¨é‡‘èå› å­åˆ†æä¸­çš„ä¼˜åŠ¿ã€‚"
        }
    ]
    
    try:
        response = llm_manager.chat_completion(
            messages=messages,
            model=llm_manager.get_model('deepseek'),
            max_tokens=200
        )
        
        print("âœ… DeepSeek V3 å“åº”:")
        print("-" * 70)
        print(response['choices'][0]['message']['content'])
        print("-" * 70)
        print(f"Tokenä½¿ç”¨: {response['usage']['total_tokens']}")
        print()
    except Exception as e:
        print(f"âŒ DeepSeek V3 è°ƒç”¨å¤±è´¥: {e}")
        print()
    
    print("[4/5] æµ‹è¯•Qwen 2.5ï¼ˆä¸­æ–‡ç†è§£ï¼‰...")
    messages = [
        {
            "role": "user",
            "content": "è¯·ç®€è¦è¯´æ˜Qwenåœ¨å¤„ç†ä¸­æ–‡é‡‘èæ–‡æœ¬åˆ†ææ—¶çš„ä¼˜åŠ¿ã€‚"
        }
    ]
    
    try:
        response = llm_manager.chat_completion(
            messages=messages,
            model=llm_manager.get_model('qwen'),
            max_tokens=200
        )
        
        print("âœ… Qwen 2.5 å“åº”:")
        print("-" * 70)
        print(response['choices'][0]['message']['content'])
        print("-" * 70)
        print(f"Tokenä½¿ç”¨: {response['usage']['total_tokens']}")
        print()
    except Exception as e:
        print(f"âŒ Qwen 2.5 è°ƒç”¨å¤±è´¥: {e}")
        print()
    
    print("[5/5] æŸ¥çœ‹APIå¯†é’¥çŠ¶æ€...")
    status = llm_manager.get_status()
    print("APIå¯†é’¥çŠ¶æ€:")
    for key_status in status['key_status']:
        print(f"  å¯†é’¥: {key_status['key']}")
        print(f"    å¤±è´¥æ¬¡æ•°: {key_status['failures']}")
        print(f"    æœ€åæˆåŠŸ: {key_status['last_success']}")
    print()
    
    print("=" * 70)
    print("ğŸ‰ å¤šå¯†é’¥è´Ÿè½½å‡è¡¡æµ‹è¯•å®Œæˆï¼")
    print()
    print("åŠŸèƒ½è¯´æ˜:")
    print("  âœ… 3ä¸ªAPIå¯†é’¥è‡ªåŠ¨è½®è¯¢")
    print("  âœ… å•ä¸ªå¯†é’¥å¤±è´¥è‡ªåŠ¨åˆ‡æ¢")
    print("  âœ… æ¯ä¸ªå¯†é’¥æ”¯æŒ3æ¬¡é‡è¯•")
    print("  âœ… æ”¯æŒ4ç§é‡‘èåˆ†ææ¨¡å‹")
    print()
    print("ä½¿ç”¨ç¤ºä¾‹:")
    print("  from panda_common.llm_manager import get_llm_manager")
    print("  llm = get_llm_manager()")
    print("  response = llm.chat_completion(messages=[...])")
    print("=" * 70)
    
except ImportError as e:
    print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
    print()
    print("è¯·å®‰è£…ä¾èµ–:")
    print("  pip install openai")
except Exception as e:
    print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
    import traceback
    traceback.print_exc()
