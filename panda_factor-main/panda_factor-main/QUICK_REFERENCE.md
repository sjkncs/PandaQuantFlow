# ğŸš€ PandaFactorå¿«é€Ÿå‚è€ƒ

## âœ… ä¿®å¤å®Œæˆ

- âœ… ç§»é™¤ä»˜è´¹æ¨¡å‹ï¼ˆKimiã€Claudeï¼‰
- âœ… æ·»åŠ 4ä¸ªå…è´¹æ¨¡å‹
- âœ… æ›´æ–°APIç«¯ç‚¹
- âœ… ä¿®å¤æµ‹è¯•è„šæœ¬

---

## ğŸ¯ 4ä¸ªå…è´¹æ¨¡å‹

```
1. DeepSeek V3      â†’ å› å­ä»£ç ç”Ÿæˆ
2. Qwen 2.5 (72B)   â†’ ä¸­æ–‡å¸‚åœºåˆ†æ
3. Qwen Coder (32B) â†’ ç®—æ³•å®ç°
4. GLM-4 (9B)       â†’ é€šç”¨å¯¹è¯
```

---

## ğŸš€ ç«‹å³å¼€å§‹

### 1. é‡å¯æœåŠ¡
```powershell
py start_server_fixed.py
```

### 2. æµ‹è¯•æ¨¡å‹
```powershell
py test_llm_multi_key.py
```

### 3. è®¿é—®ç•Œé¢
```
http://127.0.0.1:8111/factor/
```

---

## ğŸ“¡ APIç«¯ç‚¹

### è·å–æ¨¡å‹åˆ—è¡¨
```bash
GET http://127.0.0.1:8111/llm/models
```

### åˆ‡æ¢æ¨¡å‹
```bash
POST http://127.0.0.1:8111/llm/switch_model
Content-Type: application/json

{
  "model_type": "deepseek"  # æˆ– qwen, qwen_coder, glm
}
```

### æŸ¥çœ‹çŠ¶æ€
```bash
GET http://127.0.0.1:8111/llm/status
```

---

## ğŸ’» ä»£ç ä½¿ç”¨

### åŸºç¡€ç”¨æ³•
```python
from panda_common.llm_manager import get_llm_manager

llm = get_llm_manager()
response = llm.chat_completion(
    messages=[{"role": "user", "content": "ä½ çš„é—®é¢˜"}]
)
```

### æŒ‡å®šæ¨¡å‹
```python
# DeepSeek - ä»£ç ç”Ÿæˆ
response = llm.chat_completion(
    messages=[{"role": "user", "content": "å†™RSIå› å­"}],
    model=llm.get_model('deepseek')
)

# Qwen - å¸‚åœºåˆ†æ
response = llm.chat_completion(
    messages=[{"role": "user", "content": "åˆ†æå¸‚åœº"}],
    model=llm.get_model('qwen')
)
```

---

## ğŸ¨ UIä¼˜åŒ–

è¯¦è§ `UI_OPTIMIZATION_GUIDE.md`

ä¸»è¦æ”¹è¿›ï¼š
- å¡ç‰‡å¼å¸ƒå±€
- æµ®åŠ¨èŠå¤©çª—å£
- æ¸å˜è‰²è®¾è®¡
- å®æ—¶çŠ¶æ€æŒ‡ç¤º

---

## ğŸ“š å®Œæ•´æ–‡æ¡£

1. `FIXES_AND_UPDATES.md` - ä¿®å¤æ€»ç»“
2. `UI_OPTIMIZATION_GUIDE.md` - UIä¼˜åŒ–æŒ‡å—
3. `FINAL_SUMMARY.md` - å®Œæ•´æ€»ç»“
4. `QUICK_START_LLM.md` - LLMå¿«é€Ÿå¼€å§‹

---

## âœ… æ ¸å¿ƒç‰¹æ€§

- âœ… 3ä¸ªAPIå¯†é’¥å¹¶è”
- âœ… 4ä¸ªå…è´¹æ¨¡å‹
- âœ… è‡ªåŠ¨è½®è¯¢
- âœ… æ•…éšœè½¬ç§»
- âœ… å®Œå…¨å…è´¹

**ğŸ‰ å¼€å§‹ä½¿ç”¨å…è´¹LLMè¿›è¡Œå› å­å¼€å‘ï¼**
