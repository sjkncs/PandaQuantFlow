"""
å› å­åº“ä¸è‡ªç›‘ç£å­¦ä¹ é›†æˆç¤ºä¾‹
Factor Library + Self-Supervised Learning Integration

å±•ç¤ºå¦‚ä½•å°†PandaAIå› å­åº“ä¸è‡ªç›‘ç£å­¦ä¹ æ¡†æ¶ç»“åˆ
å®ç°ç«¯åˆ°ç«¯çš„é‡åŒ–å› å­æŒ–æ˜æµç¨‹
"""

import sys
sys.path.append('..')
import pandas as pd
import numpy as np
import torch
from factor_library import FactorLibrary
from ssl.contrastive import SimpleContrastiveLearning

print("="*70)
print("å› å­åº“ + è‡ªç›‘ç£å­¦ä¹  é›†æˆç¤ºä¾‹")
print("="*70)

# ==================== 1. å‡†å¤‡æ•°æ® ====================
print("\n[æ­¥éª¤ 1/5] å‡†å¤‡å¸‚åœºæ•°æ®...")

# æ¨¡æ‹Ÿä¸€å¹´çš„å¸‚åœºæ•°æ®
dates = pd.date_range('2023-01-01', periods=252)
np.random.seed(42)

# ç”Ÿæˆæ¨¡æ‹Ÿä»·æ ¼æ•°æ®ï¼ˆéšæœºæ¸¸èµ°ï¼‰
price_base = 100
prices = [price_base]
for _ in range(251):
    change = np.random.randn() * 2  # æ¯æ—¥æ³¢åŠ¨
    prices.append(prices[-1] * (1 + change/100))

data = pd.DataFrame({
    'close': prices,
    'open': [p * (1 + np.random.randn()*0.01) for p in prices],
    'high': [p * (1 + abs(np.random.randn())*0.02) for p in prices],
    'low': [p * (1 - abs(np.random.randn())*0.02) for p in prices],
    'volume': np.random.randint(1000000, 10000000, 252)
}, index=dates)

print(f"âœ… æ•°æ®å‡†å¤‡å®Œæˆ: {len(data)} ä¸ªäº¤æ˜“æ—¥")
print(f"   ä»·æ ¼èŒƒå›´: {data['close'].min():.2f} - {data['close'].max():.2f}")
print(f"   å¹³å‡æˆäº¤é‡: {data['volume'].mean():.0f}")

# ==================== 2. è®¡ç®—æŠ€æœ¯å› å­ ====================
print("\n[æ­¥éª¤ 2/5] è®¡ç®—æŠ€æœ¯å› å­...")

close = FactorLibrary.CLOSE(data)
volume = FactorLibrary.VOLUME(data)

# è®¡ç®—å¤šä¸ªæŠ€æœ¯å› å­
factors = pd.DataFrame({
    # è¶‹åŠ¿å› å­
    'ma5': FactorLibrary.MA(close, 5),
    'ma10': FactorLibrary.MA(close, 10),
    'ma20': FactorLibrary.MA(close, 20),
    'ma60': FactorLibrary.MA(close, 60),
    
    # åŠ¨é‡å› å­
    'roc5': FactorLibrary.ROC(close, 5),
    'roc10': FactorLibrary.ROC(close, 10),
    'roc20': FactorLibrary.ROC(close, 20),
    
    # æ³¢åŠ¨ç‡å› å­
    'std10': FactorLibrary.STD(close, 10),
    'std20': FactorLibrary.STD(close, 20),
    'atr14': FactorLibrary.ATR(data, 14),
    
    # æŠ€æœ¯æŒ‡æ ‡
    'rsi14': FactorLibrary.RSI(close, 14),
    'macd': FactorLibrary.MACD(close),
    
    # é‡ä»·å› å­
    'volume_ma20': FactorLibrary.MA(volume, 20),
    'volume_std20': FactorLibrary.STD(volume, 20),
    'corr_pv_20': FactorLibrary.CORRELATION(close, volume, 20),
    
    # å¸ƒæ—å¸¦
    'boll_upper': FactorLibrary.BOLL_UPPER(close, 20, 2),
    'boll_lower': FactorLibrary.BOLL_LOWER(close, 20, 2),
    
    # ä»·æ ¼ä½ç½®
    'price_position': (close - FactorLibrary.MA(close, 20)) / FactorLibrary.STD(close, 20),
})

# å»é™¤NaNå€¼
factors = factors.fillna(method='bfill').fillna(0)

print(f"âœ… å› å­è®¡ç®—å®Œæˆ: {factors.shape[1]} ä¸ªå› å­")
print(f"   å› å­åˆ—è¡¨: {list(factors.columns[:5])}... (å…±{len(factors.columns)}ä¸ª)")

# ==================== 3. å› å­æ ‡å‡†åŒ– ====================
print("\n[æ­¥éª¤ 3/5] å› å­æ ‡å‡†åŒ–...")

# Z-scoreæ ‡å‡†åŒ–
factors_normalized = (factors - factors.mean()) / factors.std()
factors_normalized = factors_normalized.fillna(0)

print(f"âœ… æ ‡å‡†åŒ–å®Œæˆ")
print(f"   å‡å€¼: {factors_normalized.mean().mean():.4f}")
print(f"   æ ‡å‡†å·®: {factors_normalized.std().mean():.4f}")

# ==================== 4. è‡ªç›‘ç£é¢„è®­ç»ƒ ====================
print("\n[æ­¥éª¤ 4/5] è‡ªç›‘ç£é¢„è®­ç»ƒ...")

# è½¬æ¢ä¸ºPyTorchå¼ é‡
factor_tensor = torch.FloatTensor(factors_normalized.values)

# åˆ›å»ºè‡ªç›‘ç£å­¦ä¹ æ¨¡å‹
input_dim = factors.shape[1]
ssl_model = SimpleContrastiveLearning(
    input_dim=input_dim,
    hidden_dim=64,
    output_dim=32
)

print(f"   æ¨¡å‹æ¶æ„: {input_dim} -> 64 -> 32")

# è®­ç»ƒå‚æ•°
num_epochs = 50
batch_size = 32

# è®­ç»ƒå¾ªç¯
losses = []
for epoch in range(num_epochs):
    epoch_losses = []
    
    # éšæœºé‡‡æ ·æ‰¹æ¬¡
    indices = torch.randperm(len(factor_tensor))
    
    for i in range(0, len(indices), batch_size):
        batch_indices = indices[i:i+batch_size]
        if len(batch_indices) < 2:
            continue
        
        batch_data = factor_tensor[batch_indices]
        
        # æ•°æ®å¢å¼ºï¼šæ·»åŠ å™ªå£°
        x1 = batch_data + torch.randn_like(batch_data) * 0.1
        x2 = batch_data + torch.randn_like(batch_data) * 0.1
        
        # è®­ç»ƒä¸€æ­¥
        loss = ssl_model.train_step(x1, x2)
        epoch_losses.append(loss)
    
    avg_loss = np.mean(epoch_losses)
    losses.append(avg_loss)
    
    if (epoch + 1) % 10 == 0:
        print(f"   Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}")

print(f"âœ… é¢„è®­ç»ƒå®Œæˆ")
print(f"   æœ€ç»ˆæŸå¤±: {losses[-1]:.4f}")
print(f"   æŸå¤±ä¸‹é™: {(losses[0] - losses[-1])/losses[0]*100:.1f}%")

# ==================== 5. å› å­è¡¨ç¤ºå­¦ä¹  ====================
print("\n[æ­¥éª¤ 5/5] æå–å› å­è¡¨ç¤º...")

# ä½¿ç”¨è®­ç»ƒå¥½çš„ç¼–ç å™¨æå–å› å­è¡¨ç¤º
with torch.no_grad():
    factor_embeddings = ssl_model.encode(factor_tensor)

print(f"âœ… å› å­è¡¨ç¤ºæå–å®Œæˆ")
print(f"   åŸå§‹ç»´åº¦: {input_dim}")
print(f"   å‹ç¼©ç»´åº¦: {factor_embeddings.shape[1]}")
print(f"   å‹ç¼©æ¯”: {input_dim/factor_embeddings.shape[1]:.1f}x")

# ==================== 6. ç»“æœåˆ†æ ====================
print("\n" + "="*70)
print("ğŸ“Š ç»“æœåˆ†æ")
print("="*70)

# è®¡ç®—å› å­ç›¸å…³æ€§
factor_corr = factors_normalized.corr()
avg_corr = factor_corr.abs().mean().mean()

print(f"\n1. åŸå§‹å› å­ç»Ÿè®¡:")
print(f"   - å› å­æ•°é‡: {factors.shape[1]}")
print(f"   - å¹³å‡ç›¸å…³æ€§: {avg_corr:.3f}")
print(f"   - é«˜ç›¸å…³å› å­å¯¹: {(factor_corr.abs() > 0.8).sum().sum() - len(factors.columns)}")

# è®¡ç®—åµŒå…¥ç›¸å…³æ€§
embedding_df = pd.DataFrame(factor_embeddings.numpy())
embedding_corr = embedding_df.corr()
avg_embedding_corr = embedding_corr.abs().mean().mean()

print(f"\n2. å­¦ä¹ åˆ°çš„è¡¨ç¤ºç»Ÿè®¡:")
print(f"   - è¡¨ç¤ºç»´åº¦: {factor_embeddings.shape[1]}")
print(f"   - å¹³å‡ç›¸å…³æ€§: {avg_embedding_corr:.3f}")
print(f"   - ä¿¡æ¯å‹ç¼©: {(1 - avg_embedding_corr/avg_corr)*100:.1f}% å»ç›¸å…³")

# è®¡ç®—æ”¶ç›Šé¢„æµ‹èƒ½åŠ›ï¼ˆç®€å•ç¤ºä¾‹ï¼‰
future_returns = (close.shift(-5) / close - 1).fillna(0)  # æœªæ¥5æ—¥æ”¶ç›Š

# ä½¿ç”¨åŸå§‹å› å­
factor_signal = factors_normalized.mean(axis=1)
signal_return_corr = np.corrcoef(factor_signal[:-5], future_returns[:-5])[0, 1]

# ä½¿ç”¨å­¦ä¹ åˆ°çš„è¡¨ç¤º
embedding_signal = pd.Series(factor_embeddings[:, 0].numpy(), index=factors.index)
embedding_return_corr = np.corrcoef(embedding_signal[:-5], future_returns[:-5])[0, 1]

print(f"\n3. é¢„æµ‹èƒ½åŠ›å¯¹æ¯”:")
print(f"   - åŸå§‹å› å­ vs æœªæ¥æ”¶ç›Šç›¸å…³æ€§: {signal_return_corr:.4f}")
print(f"   - å­¦ä¹ è¡¨ç¤º vs æœªæ¥æ”¶ç›Šç›¸å…³æ€§: {embedding_return_corr:.4f}")
print(f"   - æ”¹è¿›: {(abs(embedding_return_corr) - abs(signal_return_corr))*100:.2f}%")

# ==================== 7. åº”ç”¨å»ºè®® ====================
print("\n" + "="*70)
print("ğŸ’¡ åº”ç”¨å»ºè®®")
print("="*70)

print("""
1. å› å­æŒ–æ˜æµç¨‹:
   âœ… è®¡ç®—å¤§é‡æŠ€æœ¯å› å­ (100+)
   âœ… ä½¿ç”¨è‡ªç›‘ç£å­¦ä¹ é™ç»´å»å™ª
   âœ… æå–ä½ç»´åº¦é«˜è´¨é‡è¡¨ç¤º
   âœ… ç”¨äºä¸‹æ¸¸é¢„æµ‹ä»»åŠ¡

2. ä¼˜åŠ¿:
   âœ… æ— éœ€æ ‡æ³¨æ•°æ®
   âœ… è‡ªåŠ¨å‘ç°å› å­é—´å…³ç³»
   âœ… é™ä½è¿‡æ‹Ÿåˆé£é™©
   âœ… æé«˜è®¡ç®—æ•ˆç‡

3. ä¸‹ä¸€æ­¥:
   âœ… å¢åŠ æ›´å¤šå› å­ (Alpha101, Alpha191ç­‰)
   âœ… å°è¯•ä¸åŒè‡ªç›‘ç£æ–¹æ³• (MaskedAE, Temporal)
   âœ… åŠ å…¥å°‘é‡æ ‡æ³¨æ•°æ®å¾®è°ƒ
   âœ… å›æµ‹éªŒè¯ç­–ç•¥æ•ˆæœ
""")

print("\n" + "="*70)
print("ğŸ‰ ç¤ºä¾‹å®Œæˆï¼")
print("="*70)
